# ====================================================================
# Logstash Pipeline Configuration for SKC BI Dashboard
# Task 41.3: Centralized log processing and enrichment
# ====================================================================

input {
  # ================================================================
  # FILEBEAT INPUT - Collects logs from various sources
  # ================================================================
  beats {
    port => 5044
    client_inactivity_timeout => 60
    include_codec_tag => false
  }

  # ================================================================
  # SYSLOG INPUT - System logs
  # ================================================================
  syslog {
    port => 5000
    facility_labels => [
      "kernel", "user", "mail", "daemon", "auth", "syslog",
      "lpr", "news", "uucp", "cron", "authpriv", "ftp",
      "local0", "local1", "local2", "local3", "local4", "local5", "local6", "local7"
    ]
  }

  # ================================================================
  # DOCKER LOGS INPUT - Container logs
  # ================================================================
  gelf {
    port => 12201
    use_udp => true
    strip_leading_underscore => true
  }

  # ================================================================
  # HTTP INPUT - Application logs via REST API
  # ================================================================
  http {
    port => 8080
    codec => json
    additional_codecs => {
      "application/json" => "json"
      "text/plain" => "plain"
    }
  }

  # ================================================================
  # TCP INPUT - Direct log streaming
  # ================================================================
  tcp {
    port => 9999
    codec => json_lines
  }
}

# ====================================================================
# FILTER SECTION - Log processing and enrichment
# ====================================================================
filter {
  # ================================================================
  # COMMON ENRICHMENT - Add metadata to all logs
  # ================================================================
  mutate {
    add_field => {
      "[@metadata][pipeline]" => "main"
      "[@metadata][index_prefix]" => "skc-bi-dashboard"
      "ingestion_timestamp" => "%{@timestamp}"
    }
  }

  # ================================================================
  # FILEBEAT LOG PROCESSING
  # ================================================================
  if [input][type] == "filebeat" {
    # Parse log file paths to extract service information
    grok {
      match => { 
        "[log][file][path]" => "/var/log/(?<service_name>[^/]+)/(?<log_type>[^/]+)\.log"
      }
      tag_on_failure => ["_grok_filebeat_path_failure"]
    }

    # Set service name from filebeat fields
    if [fields][service] {
      mutate {
        add_field => { "service_name" => "%{[fields][service]}" }
      }
    }
  }

  # ================================================================
  # NEXT.JS APPLICATION LOG PROCESSING
  # ================================================================
  if [service_name] == "nextjs" or [source] =~ /nextjs/ or [container][name] =~ /nextjs/ {
    # Parse Next.js application logs
    if [message] =~ /^\{.*\}$/ {
      # Parse JSON logs
      json {
        source => "message"
        target => "app"
        add_tag => ["json_parsed"]
      }
    } else {
      # Parse text logs with grok patterns
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:component}\] %{GREEDYDATA:log_message}"
        }
        tag_on_failure => ["_grok_nextjs_failure"]
      }
    }

    # Standardize log levels
    if [level] {
      mutate {
        lowercase => ["level"]
      }
    }

    # Parse HTTP request logs
    if [message] =~ /HTTP|GET|POST|PUT|DELETE/ {
      grok {
        match => { 
          "message" => "%{WORD:method} %{URIPATH:endpoint}(?:%{URIPARAM:params})? %{NUMBER:status_code} %{NUMBER:response_time:float}ms"
        }
        add_tag => ["http_request"]
        tag_on_failure => ["_grok_http_failure"]
      }

      # Categorize HTTP status codes
      if [status_code] {
        if [status_code] >= 200 and [status_code] < 300 {
          mutate { add_field => { "response_category" => "success" } }
        } else if [status_code] >= 300 and [status_code] < 400 {
          mutate { add_field => { "response_category" => "redirect" } }
        } else if [status_code] >= 400 and [status_code] < 500 {
          mutate { add_field => { "response_category" => "client_error" } }
        } else if [status_code] >= 500 {
          mutate { add_field => { "response_category" => "server_error" } }
        }
      }
    }

    # Parse database query logs
    if [message] =~ /SQL|Query|Database/ {
      grok {
        match => { 
          "message" => "Query: %{GREEDYDATA:sql_query} Duration: %{NUMBER:query_duration:float}ms"
        }
        add_tag => ["database_query"]
        tag_on_failure => ["_grok_db_failure"]
      }
    }

    # Parse authentication logs
    if [message] =~ /login|auth|session/ {
      grok {
        match => { 
          "message" => "%{WORD:auth_action} %{WORD:auth_result} for user %{DATA:user_id}"
        }
        add_tag => ["authentication"]
        tag_on_failure => ["_grok_auth_failure"]
      }
    }

    mutate {
      add_field => { "service_type" => "application" }
      add_field => { "technology" => "nextjs" }
    }
  }

  # ================================================================
  # DOCKER CONTAINER LOG PROCESSING
  # ================================================================
  if [container] {
    # Extract container metadata
    mutate {
      add_field => { 
        "container_name" => "%{[container][name]}"
        "container_image" => "%{[container][image]}"
        "service_type" => "container"
      }
    }

    # Parse container logs based on image type
    if [container][image] =~ /postgres/ {
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:process_id}\] %{WORD:level}: %{GREEDYDATA:log_message}"
        }
        add_tag => ["postgres_log"]
        tag_on_failure => ["_grok_postgres_failure"]
      }
      mutate { add_field => { "technology" => "postgresql" } }
    }

    if [container][image] =~ /redis/ {
      grok {
        match => { 
          "message" => "%{NUMBER:process_id}:%{WORD:role} %{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:log_message}"
        }
        add_tag => ["redis_log"]
        tag_on_failure => ["_grok_redis_failure"]
      }
      mutate { add_field => { "technology" => "redis" } }
    }

    if [container][image] =~ /nginx/ {
      grok {
        match => { 
          "message" => "%{COMBINEDAPACHELOG}"
        }
        add_tag => ["nginx_access_log"]
        tag_on_failure => ["_grok_nginx_failure"]
      }
      mutate { add_field => { "technology" => "nginx" } }
    }
  }

  # ================================================================
  # SYSTEM LOG PROCESSING
  # ================================================================
  if [program] {
    mutate {
      add_field => { 
        "service_name" => "%{program}"
        "service_type" => "system"
      }
    }

    # Parse kernel logs
    if [program] == "kernel" {
      grok {
        match => { 
          "message" => "\[%{NUMBER:kernel_timestamp}\] %{GREEDYDATA:kernel_message}"
        }
        add_tag => ["kernel_log"]
      }
    }

    # Parse SSH logs
    if [program] == "sshd" {
      grok {
        match => { 
          "message" => "%{WORD:ssh_action} %{WORD:ssh_result} for %{DATA:username} from %{IP:source_ip}"
        }
        add_tag => ["ssh_log", "security"]
        tag_on_failure => ["_grok_ssh_failure"]
      }
    }

    # Parse systemd logs
    if [program] == "systemd" {
      grok {
        match => { 
          "message" => "%{WORD:systemd_action} %{DATA:unit_name}: %{GREEDYDATA:systemd_message}"
        }
        add_tag => ["systemd_log"]
        tag_on_failure => ["_grok_systemd_failure"]
      }
    }
  }

  # ================================================================
  # SECURITY LOG PROCESSING
  # ================================================================
  if "security" in [tags] or [message] =~ /fail|error|attack|breach|intrusion/ {
    # Extract IP addresses for security analysis
    if [source_ip] {
      # GeoIP lookup for source IP
      geoip {
        source => "source_ip"
        target => "geoip"
        add_tag => ["geoip"]
      }

      # Check if IP is in private network
      if [source_ip] =~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/ {
        mutate { add_field => { "ip_type" => "private" } }
      } else {
        mutate { add_field => { "ip_type" => "public" } }
      }
    }

    # Security event classification
    if [message] =~ /password|passwd/ {
      mutate { add_tag => ["password_related"] }
    }
    if [message] =~ /sudo|su / {
      mutate { add_tag => ["privilege_escalation"] }
    }
    if [message] =~ /fail|failed/ {
      mutate { add_tag => ["failure"] }
    }
  }

  # ================================================================
  # PERFORMANCE METRICS EXTRACTION
  # ================================================================
  if [response_time] {
    # Categorize response times
    if [response_time] < 100 {
      mutate { add_field => { "performance_category" => "fast" } }
    } else if [response_time] < 500 {
      mutate { add_field => { "performance_category" => "normal" } }
    } else if [response_time] < 2000 {
      mutate { add_field => { "performance_category" => "slow" } }
    } else {
      mutate { add_field => { "performance_category" => "very_slow" } }
    }
  }

  # ================================================================
  # TIMESTAMP STANDARDIZATION
  # ================================================================
  if [timestamp] and [timestamp] != [@timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
      add_tag => ["timestamp_parsed"]
    }
  }

  # ================================================================
  # FIELD CLEANUP AND STANDARDIZATION
  # ================================================================
  mutate {
    # Remove temporary fields
    remove_field => ["@version", "host", "tags"]
    
    # Ensure consistent field naming
    rename => {
      "log_message" => "message_parsed"
    }
    
    # Add environment information
    add_field => {
      "environment" => "production"
      "application" => "skc-bi-dashboard"
    }
  }

  # Remove empty fields
  ruby {
    code => "
      event.to_hash.each { |k, v|
        if v.nil? || (v.respond_to?(:empty?) && v.empty?)
          event.remove(k)
        end
      }
    "
  }
}

# ====================================================================
# OUTPUT SECTION - Send processed logs to Elasticsearch
# ====================================================================
output {
  # ================================================================
  # ELASTICSEARCH OUTPUT - Primary log storage
  # ================================================================
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Dynamic index naming based on service and date
    index => "%{[@metadata][index_prefix]}-%{service_type}-%{+YYYY.MM.dd}"
    
    # Document type and ID
    document_type => "_doc"
    document_id => "%{[@metadata][beat]}-%{[@metadata][version]}-%{offset}"
    
    # Template management
    template_name => "skc-bi-dashboard"
    template_pattern => "skc-bi-dashboard-*"
    template_overwrite => true
    
    # Performance settings
    flush_size => 500
    idle_flush_time => 1
    workers => 2
    
    # Retry settings
    retry_on_conflict => 3
    retry_max_interval => 5
    
    # Health check
    healthcheck_path => "/_cluster/health"
  }

  # ================================================================
  # DEAD LETTER QUEUE - Failed processing
  # ================================================================
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "skc-bi-dashboard-failed-%{+YYYY.MM.dd}"
      document_type => "_doc"
      
      # Add failure metadata
      manage_template => false
    }
  }

  # ================================================================
  # SECURITY LOGS - Special handling
  # ================================================================
  if "security" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "skc-bi-dashboard-security-%{+YYYY.MM.dd}"
      document_type => "_doc"
      flush_size => 100
      idle_flush_time => 5
    }
  }

  # ================================================================
  # DEBUG OUTPUT - Development only
  # ================================================================
  if [@metadata][pipeline] == "debug" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }

  # ================================================================
  # METRICS OUTPUT - Performance monitoring
  # ================================================================
  if [response_time] or [query_duration] {
    statsd {
      host => "statsd"
      port => 8125
      gauge => {
        "skc.bi.response_time" => "%{response_time}"
        "skc.bi.query_duration" => "%{query_duration}"
      }
      increment => [
        "skc.bi.logs.processed",
        "skc.bi.logs.%{service_type}",
        "skc.bi.logs.%{level}"
      ]
    }
  }
} 